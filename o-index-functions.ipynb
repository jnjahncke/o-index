{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b263c1-d1ff-48b7-ae0f-0e0f261be8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from re import *\n",
    "import os\n",
    "\n",
    "# Get PMCID from PMID\n",
    "def get_pmcid(pmid):\n",
    "    base_url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    try:\n",
    "        pmcid = soup.find_all('a', {'class' : 'id-link', 'data-ga-action' : 'PMCID'})[0].text.strip()\n",
    "    except:\n",
    "        pmcid = None\n",
    "    return pmcid\n",
    "\n",
    "# Get PMIDs, Openness from author name\n",
    "def get_pmids_open(author):\n",
    "    \n",
    "    # Parse author name, build first+last\n",
    "    author = author.split()\n",
    "    if len(author) > 1:\n",
    "        aname = author[0]\n",
    "        for name in author[1:]:\n",
    "            aname += \"%\" + name\n",
    "    else:\n",
    "        aname = author[0]\n",
    "        \n",
    "    #find total number of pages\n",
    "    base_url= f'https://pubmed.ncbi.nlm.nih.gov/?term={aname}&page=1'\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pages = soup.find_all('label', {'class' : 'of-total-pages'})\n",
    "    p = pages[0].text.strip()\n",
    "    p = p.split()\n",
    "    pagenum=int(p[1])\n",
    "    \n",
    "    # scrape pubmed\n",
    "    pmids = []\n",
    "    entries = []\n",
    "    for i in range(1,pagenum+1): #change # into max num of pages\n",
    "        URL = f'https://pubmed.ncbi.nlm.nih.gov/?term={aname}&page={i}'\n",
    "        response = requests.get(URL)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pmids += soup.find_all('span', {'class' : 'docsum-pmid'})\n",
    "        entries += soup.find_all(\"div\", class_='docsum-content')\n",
    "    \n",
    "    # build dictionary of id:pmcid\n",
    "    ids = {}\n",
    "    for id,entry in zip(pmids,entries):\n",
    "        pmcid = get_pmcid(id.text.strip())\n",
    "        \n",
    "        if pmcid:\n",
    "            ids[id.text.strip()] = pmcid\n",
    "        elif search(r\"Free\",entry.text.strip()): # use regex to search for \"Free\" in docsum-content\n",
    "            ids[id.text.strip()] = \"open\"\n",
    "        else:\n",
    "            ids[id.text.strip()] = \"closed\"\n",
    "        \n",
    "    return ids\n",
    "\n",
    "def get_openness(author, api):\n",
    "    ids = get_pmids_open(author)\n",
    "    apikey = open(api, 'r').read()\n",
    "    \n",
    "    # Load keywords and create open-science categories\n",
    "    terms = pd.read_csv('keywords.csv')\n",
    "    categories = terms['category']\n",
    "    category_descriptions = terms['category_description']\n",
    "    categories_unique = np.unique(np.array(categories))\n",
    "    category_descriptions = category_descriptions.unique().tolist()\n",
    "    full_text = 'full_text'\n",
    "    category_descriptions.append(full_text)\n",
    "    \n",
    "    #create df with all the unique categories:\n",
    "    data = (len(ids), len(category_descriptions))\n",
    "    o_idx_df = pd.DataFrame(np.zeros(data), columns = category_descriptions)\n",
    "    \n",
    "    df_list = [0] * len(category_descriptions)\n",
    "    pmcids = []\n",
    "    for i, item in enumerate(ids): \n",
    "        if ids[item] == 'closed':\n",
    "            continue \n",
    "        if ids[item] == 'open':\n",
    "            o_idx_df.loc[[i],['full_text']] = 1\n",
    "        else:\n",
    "            pmcids.append(ids[item])\n",
    "            o_idx_df.loc[[i],['full_text']] = 1\n",
    "            \n",
    "    db = 'pmc'\n",
    "    base = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?'\n",
    "    dict_term = defaultdict(list)\n",
    "    fulfilled_categories = [0] * len(categories_unique)\n",
    "    for j, pmcid in enumerate(pmcids):\n",
    "        s = '{:s}db={:s}&id={:s}'.format(base, db, pmcid, apikey)\n",
    "        out = requests.get(s)\n",
    "        bs = BeautifulSoup(out.content, features=\"xml\")\n",
    "        # Check if full text is available; if not - move to the next paper\n",
    "        #full_text_available = not (bs.findAll('sec') == [])\n",
    "        #if full_text_available is True:\n",
    "        for i, categoryInd in enumerate (categories_unique):\n",
    "            found_keyword = False\n",
    "            # Loop through specific keywords related to each open-science category\n",
    "            for term in terms['keyword'][terms['category'] == categoryInd]:\n",
    "                for s in finditer(term, out.text, IGNORECASE):\n",
    "                    o_idx_df.iloc[[j],[i]] = 1\n",
    "                    found_keyword = True\n",
    "\n",
    "                # If one keyword is found, stop with searching for this category\n",
    "                if found_keyword is True:\n",
    "                    break\n",
    "    \n",
    "    return o_idx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddad9ab7-37a5-4a01-abb7-8eff8df67b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_openness\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjason early\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapikey.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 67\u001b[0m, in \u001b[0;36mget_openness\u001b[0;34m(author, api)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_openness\u001b[39m(author, api):\n\u001b[0;32m---> 67\u001b[0m     ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_pmids_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     apikey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(api, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Load keywords and create open-science categories\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 38\u001b[0m, in \u001b[0;36mget_pmids_open\u001b[0;34m(author)\u001b[0m\n\u001b[1;32m     36\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m pages \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mof-total-pages\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 38\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     39\u001b[0m p \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     40\u001b[0m pagenum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(p[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "get_openness(\"jason early\", \"apikey.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec22369-8265-46a3-9c61-816a51be2b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
