{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b18b93-fa4a-4443-8c03-afa1a365f1d3",
   "metadata": {},
   "source": [
    "# Define Functions to Get PM(C)IDs and Openness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4cb65e-ed22-4175-8a28-0ab7b6e35ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from re import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f98f47b-c779-46a8-b5b0-1508b7606246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed example:  (None, '2023')\n",
      "Open example:  ('PMC7612594', '2022')\n"
     ]
    }
   ],
   "source": [
    "# Get PMCID from PMID\n",
    "def get_pmcid_year(pmid):\n",
    "    base_url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    date = soup.find_all('span', {'class' : 'cit'})[0].text.strip().split()[0]\n",
    "    try:\n",
    "        pmcid = soup.find_all('a', {'class' : 'id-link', 'data-ga-action' : 'PMCID'})[0].text.strip()\n",
    "    except:\n",
    "        pmcid = None\n",
    "    return (pmcid , date)\n",
    "\n",
    "print(\"Closed example: \", get_pmcid_year(\"35770940\"))\n",
    "print(\"Open example: \", get_pmcid_year(\"35165460\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1addfc-0faf-4c51-820c-0a744c5bcc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jennifer:  ({'35770940': 'closed', '35944998': 'PMC9480892', '32321746': 'PMC7204083', '37540708': 'open'}, {'35770940': '2023', '35944998': '2022', '32321746': '2020', '37540708': '2023'})\n",
      "Emily L:  ({'36792646': 'PMC9932154', '36582611': 'PMC9792983', '33906174': 'PMC8504120'}, {'36792646': '2023', '36582611': '2022', '33906174': '2021'})\n",
      "Arpy:  ({'30471926': 'PMC6655561', '30096299': 'PMC6447408', '34197733': 'PMC8376805', '32999462': 'PMC7957574', '29632380': 'PMC5896795', '26220313': 'PMC4584188', '26131660': 'PMC4545963', '22660328': 'PMC3367801', '36384944': 'PMC9668842', '33230336': 'closed', '26551563': 'PMC4716836', '32613945': 'PMC7360370', '25723967': 'PMC4371381', '26905595': 'PMC4764347', '25739505': 'PMC4425585', '21825165': 'PMC3174680', '28384468': 'PMC5439268', '22866029': 'PMC3406316', '19056989': 'closed', '23403489': 'PMC3566411', '22325203': 'PMC3278709', '26104011': 'closed'}, {'30471926': '2019', '30096299': '2018', '34197733': '2021', '32999462': '2020', '29632380': '2018', '26220313': '2016', '26131660': '2015', '22660328': '2012', '36384944': '2022', '33230336': '2020', '26551563': '2015', '32613945': '2020', '25723967': '2015', '26905595': '2016', '25739505': '2015', '21825165': '2011', '28384468': '2017', '22866029': '2012', '19056989': '2008', '23403489': '2013', '22325203': '2012', '26104011': '2015'})\n"
     ]
    }
   ],
   "source": [
    "# Get list of PMIDs from author name\n",
    "def get_pmids_open(author):\n",
    "    \n",
    "    # Parse author name, build first+last\n",
    "    author = author.split()\n",
    "    if len(author) > 1:\n",
    "        aname = author[0]\n",
    "        for name in author[1:]:\n",
    "            aname += \"%\" + name\n",
    "    else:\n",
    "        aname = author[0]\n",
    "        \n",
    "    #find total number of pages\n",
    "    base_url= f'https://pubmed.ncbi.nlm.nih.gov/?term={aname}&page='\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pages = soup.find_all('label', {'class' : 'of-total-pages'})\n",
    "    p = pages[0].text.strip()\n",
    "    p = p.split()\n",
    "    pagenum=int(p[1])\n",
    "    \n",
    "    # scrape pubmed\n",
    "    pmids = []\n",
    "    entries = []\n",
    "    for i in range(1,pagenum+1): #change # into max num of pages\n",
    "        URL = f'https://pubmed.ncbi.nlm.nih.gov/?term={aname}&page={i}'\n",
    "        response = requests.get(URL)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pmids += soup.find_all('span', {'class' : 'docsum-pmid'})\n",
    "        entries += soup.find_all(\"div\", class_='docsum-content')\n",
    "    \n",
    "    # build dictionary of id:pmcid\n",
    "    ids = {}\n",
    "    years = {}\n",
    "    for id,entry in zip(pmids,entries):\n",
    "        pmcid, year = get_pmcid_year(id.text.strip())\n",
    "        years[id.text.strip()] = year\n",
    "        if pmcid:\n",
    "            ids[id.text.strip()] = pmcid\n",
    "        elif search(r\"Free\",entry.text.strip()): # use regex to search for \"Free\" in docsum-content\n",
    "            ids[id.text.strip()] = \"open\"\n",
    "        else:\n",
    "            ids[id.text.strip()] = \"closed\"\n",
    "        \n",
    "    return ids, years\n",
    "\n",
    "print(\"Jennifer: \", get_pmids_open(\"jennifer jahncke\"))\n",
    "print(\"Emily L: \", get_pmids_open(\"emily lecy\"))\n",
    "print(\"Arpy: \", get_pmids_open(\"arpiar saunders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e2f751d-3348-4eca-97f9-c4e5c57d2476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'terms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 70\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m    \n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m o_idx_df\n\u001b[0;32m---> 70\u001b[0m \u001b[43mget_openness\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfranz weber\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../apikey.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 54\u001b[0m, in \u001b[0;36mget_openness\u001b[0;34m(author, api)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keyword_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m'\u001b[39m][keyword_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m cat]:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m finditer(keyword, out\u001b[38;5;241m.\u001b[39mtext, IGNORECASE):\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m#if we are on cat \"code relevant\" (cat 3) we are testing to see if code is relevant for this paper\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m         o_idx_df\u001b[38;5;241m.\u001b[39miloc[[j],[i]] \u001b[38;5;241m=\u001b[39m \u001b[43mterms\u001b[49m\u001b[38;5;241m.\u001b[39mloc[k][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     55\u001b[0m         found_keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# If one keyword is found, stop with searching for this category\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'terms' is not defined"
     ]
    }
   ],
   "source": [
    "def get_openness(author, api):\n",
    "    ids, years = get_pmids_open(author)\n",
    "    \n",
    "    apikey = open(api, 'r').read()\n",
    "    \n",
    "  # Load keywords and create open-science categories\n",
    "    keyword_df = pd.read_csv('keywords.csv')\n",
    "    categoryIDs = np.unique(np.array(keyword_df['category']))\n",
    "    category_descriptions = keyword_df['category_description']\n",
    "    category_descriptions = category_descriptions.unique().tolist()\n",
    "    full_text = 'full_text'\n",
    "    category_descriptions.append(full_text)\n",
    "    \n",
    "    #create df with all the unique categories:\n",
    "    #data = [[None] * len(ids) for _ in range(len(category_descriptions))]\n",
    "    data = (len(ids), len(category_descriptions))\n",
    "\n",
    "    #data = (len(ids), len(category_descriptions))\n",
    "    o_idx_df = pd.DataFrame(np.zeros(data), columns = category_descriptions)\n",
    "    #deleting the 'code relevant column from the final df\n",
    "    del o_idx_df['code_relevant']\n",
    "    pmcids = []\n",
    "    for i, item in enumerate(ids): \n",
    "        o_idx_df.loc[[i],['pmid']] = item\n",
    "        o_idx_df.loc[[i],['year']] = years[item]\n",
    "        if ids[item] == 'closed':\n",
    "            pmcids.append(None)\n",
    "            #if PMCID is unavailable make items in df None type\n",
    "            o_idx_df.iloc[[i],0:2] = None\n",
    "            continue\n",
    "        if ids[item] == 'open':\n",
    "            pmcids.append(None)\n",
    "            o_idx_df.loc[[i],['full_text']] = 1\n",
    "            o_idx_df.iloc[[i],0:2] = None\n",
    "\n",
    "        else:\n",
    "            pmcids.append(ids[item])\n",
    "            o_idx_df.loc[[i],['full_text']] = 1\n",
    "    db = 'pmc'\n",
    "    base = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?'\n",
    "    for j, pmcid in enumerate(pmcids):\n",
    "        if pmcid == None:\n",
    "            continue\n",
    "        s = '{:s}db={:s}&id={:s}'.format(base, db, pmcid, apikey)\n",
    "        out = requests.get(s)\n",
    "        bs = BeautifulSoup(out.content, features=\"xml\")\n",
    "        # Check if full text is available; if not - move to the next paper\n",
    "        for cat in categoryIDs:\n",
    "            found_keyword = False\n",
    "            # Loop through specific keywords related to each open-science category\n",
    "            for keyword in keyword_df['keyword'][keyword_df['category'] == cat]:\n",
    "                for s in finditer(keyword, out.text, IGNORECASE):\n",
    "                    #if we are on cat \"code relevant\" (cat 3) we are testing to see if code is relevant for this paper\n",
    "                    o_idx_df.iloc[[j],[i]] = terms.loc[k][\"weight\"]\n",
    "                    found_keyword = True\n",
    "\n",
    "                # If one keyword is found, stop with searching for this category\n",
    "                if found_keyword is True:\n",
    "                    break\n",
    "            #if code category is 0, check to see if code is relevant\n",
    "            if cat == 2 and found_keyword== True:\n",
    "                break \n",
    "            if cat == 3 and found_keyword == False:\n",
    "                o_idx_df.iloc[[j],[i-2]] = None\n",
    "                break    \n",
    "                \n",
    "    \n",
    "    return o_idx_df\n",
    "\n",
    "get_openness(\"franz weber\", \"../apikey.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
